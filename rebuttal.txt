We thank the reviewers for all of their thoughtful feedback and insights. Reviewers’ questions are copied verbatim with heading notation “Rx.y”, where “x” is the reviewer number and “y” is the question/comment number. 
====================================================================
Reviewer: 1

Comments to the Author
Summary
Continual Learning (CL) aims to give deep learning models the capability to accumulate knowledge over time without forgetting past distributions. Most methods in CL focus on directly mitigating forgetting, either by using a regularization over the modification of the weights, a distillation function or a replay-based strategy. However, these methods do not focus on what the model is learning, which ignores that learning representations that can generalize better can indirectly improve performance and mitigate forgetting. On the other hand, Generalization methods aim to have models capable of correctly classifying out-of-distribution samples. In this paper, the authors propose using methods that increase the generalization capabilities of CL approaches. For this, the authors show in the paper that generalization methods can help reduce forgetting and achieve better out-of-distribution performance. Then, they propose a new method called Shape-Texture Consistency Regularization (STCR) that uses a style transfer operation to expand the batch and regularize the loss function. Finally, the authors show that STCR complements other CL methods and improves the performance in multiple benchmarks.

Strengths:
- Although mixing CL with generalization methods is not new, I agree with the authors that it is rare and could benefit the CL community.

- The method (STCR) proposed is an interesting approach and, as mentioned by the authors, is easily adapted to be used together with other replay methods.

- The experiments show an improvement compared to other methods. The authors did great work showing different experiments demonstrating generalization capabilities, mitigation forgetting and ablation over the hyperparameters.

Weakness
- The authors oversell their approach. STCR is not the first approach that joins the ideas of generalization and continual learning; other works have proposed the use of methods of data augmentation together with replay strategies. Also, generalization is much more than methods that increase the variability of the dataset.
   - For example, many works in continual learning focus on other ideas of generalization, like self-supervised or meta-learning.

- The proposed method uses a pre-trained model (AdaIN) for the style transfer operation. This is not a weakness, but it adds a slightly unfair comparison against other methods.

- There are some structural problems in the paper.
   - The first contribution mentioned in the introduction is the exploration of generalization and forgetting. However, Section III starts by mentioning that empirical evidence is present in the results. It could be good to present the empirical evidence and then talk about STCR; this can help tell a better story and simplify the experiments section.

- The definition of CL presented in III.A does not include replay-based strategies.

- In Table III, forgetting of row 4 is better than row 3.

- The experiment section needs to be better structured. The baselines are presented together but are used in different sections.
   - Even there are some methods with two names (LwF and KD)
   - An alternative to the current structure is:
       - Present the experiments showing that generalization can help CL before Section III.
       - Then, in the experiment section, split better what experiments are for CL and what ablation studies are.

Question to the authors:
R1.1
- Why use this version of Average Incremental Accuracy? In CL, the average accuracy of all the tasks is expected to be shown using only the final model. Some information could be lost by using the accuracy of all the models.

Thanks for the question, we have recorded the accuracy for the last model on all tasks as well, but due to there are a lot of continual learning framework using this version of Average Incremental Accuracy (Average accuracy for all the models), for example, "iCaRL: Incremental Classifier and Representation Learning".
And we value your suggestion so much, and agree with "Some information could be lost by using the accuracy of all the models.". So we added the average accuracy of all the tasks using only the final model to the appendix.


R1.2  
- Most people reading this paper could not be familiar with the generalization metrics (R-C and R-R). It could be essential to add the formulas to the paper.

Thanks for your suggestion, we’ve added the formulas of R-C and R-R to section II.

R1.3
- The style transfer operation shown in Fig 2 makes sense since the elements from the buffer have a clear style that can be transferred to the new image. However, how does it work in ImageNet, where the images do not have clear styles, just new classes? Is there an intuition for how this works when the “style” is not clearly defined, or is it defined by the class?

xxx (discuss)
Clarify the definition of style

Why we need to do a style transfer?
Add a style transfer ablation ( common style transfer, from mem, no)


R1.4 
- What happens when one applies MixUp (or another method) with the losses proposed in Eq 3?

Add MixUp to STCR. (exp)

R1.5
- How many runs or seeds do you use? There is no mention of std on the results.

Reproduce table 1 and 2( run 3 times per exp)

R1.6
- In line 570, what do you mean by: “data is augmented”?

Xxx ( ICARL, let’s change it to “ icarl has a built in data augmentation methods”?)


====================================================================
Reviewer: 2

Comments to the Author
R2.1
1. In equation (1), where do the style images come from? In the description bellowed Fig 2, the author claimed “The style images come from the examples stored in the memory buffer.” For regularization-based or architecture expansion approaches, there is also a memory buffer to store style images? The author is supposed to clarify how the style images are made.

Xxx ( I think this should be very clear already, how do we make a change?)

R2.2
2. In replay-based methods, the exemplar sets stored the train images from old classes. In part C of section III, the author claimed “we employed images from the exemplar sets as style templates to generate shape-texture conflict images.” Does the exemplar store images from old classes and style images? 

Xxx( same as first question, seems he did not get what we mean the styles are the things stored)

R2.3
3. In the description bellowed equation (3), Dt refers to shape-texture conflict images. In the explanation above, Dt refers to the current training set Dt. The author should verify the variable in the manuscript.

Change text for dt 

R2.4
4. The citation in part B of section IV.

Change text for first sentence

R2.5
5. To further verify how much the model has forgotten about the old tasks, the author can calculate the average forgetting for each task as in [a], [b].

xxx

R2.6
6. The author assigns half of the classes to the initial base task and distributes the remaining classes equally over the incremental steps. A more common protocol is the number of classes in each task is fixed and the same. For example, in cifar-100, there are 10 tasks and each task contains 10 classes.

Change exp design to this

R2.7
7. In the incremental step, the classifier is extended with the training proceed. The author can consider conducting ablation experiments on models that use a fixed-size classifier or nearest class mean classifier.
[a] Shim, Dongsub, et al. "Online class-incremental continual learning with adversarial shapley value." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 11. 2021
[b] Sun, Guanglu, et al. "CeCR: Cross-entropy contrastive replay for online class-incremental continual learning." Neural Networks (2024): 106163.

Read two papers first
Change softmax to KNN in the ablation table. 


====================================================================
Reviewer: 3

Comments to the Author
This paper proposes a perspective that emphasizes both enhancing generalization and preventing catastrophic forgetting in continual learning. The authors introduce a simple yet effective regularization technique, primarily grounded in data augmentation, to attain this objective. While the experiments demonstrate its effectiveness, there's concern that much of the improvement might stem from the augmented data, obscuring the true impact of the proposed technique.

Furthermore, the method appears to lack novelty and seems overly simplistic. When the technical contribution is limited, providing more theoretical justification could bolster its credibility. However, the current version fails to convincingly address this.

Additionally, I recommend repositioning Section V-A before Section III to bolster the overall motivation of the paper. This restructuring could enhance the coherence and strength of the argument.

Add flat minimum back.
Say why we do ablation later.

====================================================================
Reviewer: 4

Comments to the Author
This paper explores the relationship between models' generalization and forgetting ability in the scenario of continual learning. It presents empirical evidence that each field has a mutually positive effect on the other. Also, building on this finding, it proposes the STCR, which caters to continual learning, facilitating the model’s generalisation ability on ood data and forgetting ability on iid data. Finally, extensive experiments are conducted to demonstrate the effectiveness of the proposed method.

Strengths:
1) This paper studies continual learning in the context of generalization and catastrophic forgetting. The research problem is of great significance, and indeed, it is one of the earliest works on this problem.
2) The proposed method is simple and clear.
3) The writing is clear and well-structured.
4) The experiment is comprehensive.  

Weaknesses:

R4.1
1) The paper claims that "this is the first work focused on exploring the interplay of out-of-distribution generalization of the current task and forgetting of the earlier tasks.” As far as I know, there are some research before this paper.
Ref:
[1] Riemer, Matthew, et al. "Learning to learn without forgetting by maximizing transfer and minimizing interference." arXiv preprint arXiv:1810.11910 (2018).
[2] Boschini, Matteo, et al. "Transfer without forgetting." European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.
[3] Peng, Jian, et al. "Lifelong learning with cycle memory networks." IEEE Transactions on Neural Networks and Learning Systems (2023).

Xxx ( we may need to state the differences)

R4.2
2) Methods, eq(3), why the hyper-parameter of the first loss item and second loss item is equal?

Add hyper-parameter for item2, and do ablation.

R4.3
3)The authors explore the relationship between model generalisation and catastrophic forgetting. Although a method is proposed to improve models to overcome catastrophic forgetting and its effectiveness is demonstrated experimentally, the corresponding insights and mechanisms as to why this method is effective are unclear, and the proposed method lacks specificity for overcoming catastrophic forgetting.

Same as R3

R4.4
4)The authors claim that the main contribution is to reveal the relationship between forgetting and generalisation, which is exciting. However, from the experiments, it seems that only correlation analyses were done and more evidence and research needs to be shown. For example some visualisation analysis or quantitative analysis methods.

Same as R3

R4.5
5)Sec V, The comparative experiments section lacks comparisons of some of the latest methods.

Xxx ( discuss)

R4.6
6)The memory buffer size is missing in the experiment of replay-based methods. And, it is necessary to compare the performance of models under different buffer size settings, taking into account that this one factor is important for the replay-based approach.

Add memsize ablation study for STCR. And report.

R4.7
7）line 272, the ref after AdaIN is missing.

Xxx ( add ref)

====================================================================
Reviewer: 5

Comments to the Author
This paper first explores the interplay between generalization and forgetting within continual learning by conducting an empirical study. This paper further introduces a simple yet effective Shape Texture Consistency Regularization method, dubbed STCR.

My main concerns are the proposed methods:
R5.1
1. What is the connection between the empirical evidence and the proposed methods. This paper shows that OOD generalization approaches can reduce the catastrophic forgetting and CL methods can strength the OOD generalization. The proposed method seems to be motivated by these evidence, but I don't understand how the proposed method is motivated. To me, the proposed method only wants to enforce the model learning shape and texture representations for each classification task.

Xxx (same as 4.3, discuss)

R5.2
2. It is mentioned in Section 3.B that "Our STCR has to learn shape and texture representations for each classification task". Why does it have to learn shape and texture representations? Please describe why only shape and texture representations are enough for continual learning.

Xxx ( I think this has nothing to do with continual learning, it is a common technique, and shape, texture here is a relative concept)

R5.3
3. Please explain more about the style transfer operation AdaIN. It seems that the real-time style transfer approach AdaIN is trained with Irrelevant datasets, but the training datasets are unknown. How does it promise to generate correct shape-texture conflict images for current tasks?

Xxx ( we need to change this, style transfer should not do a pretrain)
Add ablation for style transfer model training data ( incremental, only t1, all data)

Other suggestions:

R5.4
1. All metrics in Section 4.C are confusing, including F, R-C, \Delta F and \Delta R-C in Section 5. Please define them by clear equations.

Xxx (same as 1.2, I will add this)

R5.5
2. Is the proposed method able to intersect with regularization-based approaches?
Xxx ( I don’t think we can do this)



====================================================================
Reviewer: 6

Comments to the Author
The paper looks at the connection between generalization and continual learning, and introduces a method for enhancing the generalization capabilities of existing continual learning baselines. It is generally well-written, but I have some comments below:

R6.1
1. In Eq. 2, what is DKL?

Xxx （ KL distance, seems he did not know the basics, but I think we just add)

R6.2
2. What is the role of the temperature hyper-parameter in Eq. 2? How does it affect the results?

Add temperature hyper-parameter ablation study.

R6.3
3. In Table III, shouldn't the F-score for gamma = 0.1 be in bold, not gamma=0.01?

Xxx (...)

R6.4
4. There should be consistency when writing R-C (it's written with different stylizations throughout the paper)

Text 

====================================================================
Reviewer: 7

Comments to the Author
R7.1
(1) The forgetting evaluation focuses on task performance drop is valuable but limited. Backward Transfer (BWT) evaluation would be a more comprehensive and widely-used metric in continual learning. It assesses not just forgetting but also potential improvement on past tasks due to encountering related information in new ones. This would be a more proper evaluation metrics to assess the stability of continual learning models. I suggest the authors consider using BWT.

Xxx ( Add BWT)

R7.2
(2) The proposed method leverages a replay-based approach. To understand the scalability of the method, it would be beneficial for the authors to analyze the model complexity. Specifically, how does the model complexity change as the number of encountered samples increases and the memory buffer grows with more saved samples.

Xxx ( can do together with 4.6)

R7.3
(3) Applying consistent regularization for improving generalization in continual learning is not a new approach. To strengthen the novelty of your work, it would be beneficial to discuss relevant existing literature that employs consistency regularization in continual learning, such as [1, 2]. This could be included in the related work section. Or you could consider comparing your model's performance to these existing methods in an experiment.

[1]Ho, Stella, et al. Semi-supervised continual learning with meta self-training. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (CIKM 2022)

[2] Shi, Yanyan, et al. Multi-granularity knowledge distillation and prototype consistency regularization for class-incremental learning. Neural Networks (2023)

Cite and add exps


====================================================================
Reviewer: 8
????
Comments to the Author
This paper proposes the prototype-based soft-label propagation (PSLP) for transductive few-shot learning. The method is interesting, which combines the advantages of prototype-based methods and label propagation-based methods, and improves the shortcomings in the class-imbalanced settings. The experiments have demonstrated the efficiency and robustness. I suggest the following points should be addressed. First, In Introduction, the author mentioned that integrate the above two principles, please clearly state the two principles. It is better to discuss the differences of ProtoLP[16] and the propose method before the detail of the proposed method. In Figure 1, it seems the graph representation is directed. But the method uses the un directed graph representation. “Existing prototypical methods typically require accurate estimation of prototypes for each class by optimizing learnable parameters during testing, while label propagation-based approaches often necessitate the construction of a reliable graph structure and upgrade the graph structure during label propagation iteration.” Here, “while” should be changed into “and”. Some other minor comments are follows.
1.Can the author give a more explanatory architecture of the proposed method in Figure 2?
2.In the QS-Graph Refining, the author adopts the nearest neighbor graph building, can the author provide the reason of adopting the nearest neighbor graph? There are other graph building methods, can the author conduct more experiments to figure out the effect of the graph model?
3.The experiment setting needs to provide the experiment flatform.
4.Are there any experiment results demonstrating the effectiveness of the soft-label over the hard-label propagation?
5.More literatures of recent two years should be reviewed and discussed.
6.The language of the manuscript is suggested to be polished throughout.



====================================================================
Reviewer: 9

Comments to the Author
The paper attempts to connect two fields, namely generalization and preservation of existing knowledge from continual learning. The authors propose a shape-texture consistency regularization method to bridge these two fields in the context of continual learning. The method learns shape and texture representations separately from each tasks to enhance generalization and performs well over the continual learning methods.

Points -

R9.1
1. The paper connects the concept of generalization very well with continual learning.

R9.2
2. The proposed STCR method is very intuitive and interesting. The applicability of STCR to different continual learning methods is appreciable.

R9.3
3. The comparison with existing baselines provides a good benchmark for evaluation across continual learning and generalization methods.

R9.4
4. The experimental part is quite convincing. It will be interesting to see how the concept of generalization can be applied to replay-free continual learning methods[1][2][3] which are gaining a lot of attention recently. Although more forgetting is expected in replay-free settings, it would be nice to provide a solution for those settings when no exemplars are available.
[1] Goswami et al., Fecam: Exploiting the heterogeneity of class distributions in exemplar-free continual learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
[2] Petit et al., Fetril: Feature translation for exemplar-free class-incremental learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023.
[3] Zhu et al., Prototype augmentation and self-supervision for incremental learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.

Run codes get result.
